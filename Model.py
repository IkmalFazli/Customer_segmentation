# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NusifoQiifD5uhVfiYaJ3AM01z_vlIpf
"""

from sklearn.metrics import confusion_matrix, classification_report
from sklearn.preprocessing import OneHotEncoder,LabelEncoder
from sklearn.experimental import enable_iterative_imputer
from module_asessment2 import ModelCreation,Plot_features
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import TensorBoard
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import plot_model
from sklearn.impute import IterativeImputer
from sklearn.impute import KNNImputer
import matplotlib.pyplot as plt
import scipy.stats as ss
import pandas as pd 
import numpy as np
import datetime
import pickle
import os

#%% Static
DATA_TRAIN_PATH = os.path.join(os.getcwd(),'Dataset','Train.csv')
LE_PATH = os.path.join(os.getcwd(),'le.pkl')
SS_FILE_NAME = os.path.join(os.getcwd(),'standard_scaler.pkl')
log_dir = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
HEART_LOG_FOLDER_PATH = os.path.join(os.getcwd(),'logs',log_dir)
MODEL_PATH = os.path.join(os.getcwd(),'model.h5')
categorical_columns = ['job_type', 'marital','education','default',
                       'housing_loan','personal_loan','communication_type',
                       'day_of_month','month','prev_campaign_outcome',
                       'term_deposit_subscribed']
continuous_columns = ['customer_age','balance','last_contact_duration',
                      'num_contacts_in_campaign','num_contacts_prev_campaign']

# EDA
#%% Step 1- Data Loading
df = pd.read_csv(DATA_TRAIN_PATH)

#%% Step 2- Data Inspection
df.head(10)
df.tail(10)
df.info() # check for null and can check which column is categorical

df.describe().T

print(df.isna().sum())
print(df.duplicated().sum())

plot = Plot_features()
plot.plot_cat(df,categorical_columns)
plot.plot_con(df,continuous_columns)

df.groupby(['job_type','term_deposit_subscribed']).agg(
    {'term_deposit_subscribed':'count'}).plot(kind='bar')

df.groupby(['education','term_deposit_subscribed']).agg(
    {'term_deposit_subscribed':'count'}).plot(kind='bar')

df.groupby(['marital','term_deposit_subscribed']).agg(
    {'term_deposit_subscribed':'count'}).plot(kind='bar')

df.groupby(['term_deposit_subscribed','education','job_type','marital']).agg(
    {'term_deposit_subscribed':'count'})

"""# STEP 3) DATA CLEANING
- drop id
- no duplicated
- drop days_since_prev_campaign_contact because to many null."""

df = df.drop(labels=['id','days_since_prev_campaign_contact'], axis=1)

column_names = df.columns

for i in categorical_columns:
  le = LabelEncoder() #convert data jadi integer
  df[i] = le.fit_transform(df[i])
  with open(LE_PATH,'wb') as file:
    pickle.dump(le,file)

df_backup1 = df.copy()

#KNN Imputer
knn_imputer = KNNImputer(n_neighbors=5, metric='nan_euclidean')
imputed_data = knn_imputer.fit_transform(df)
df = pd.DataFrame(imputed_data)
df.columns = column_names

#II Imputer (MICE)
iimputer = IterativeImputer()
imputeddata = iimputer.fit_transform(df_backup1)
df_backup1 = pd.DataFrame(imputeddata)
df_backup1.columns = column_names

df_backup1.describe().T

df.describe().T

# Step 5- Preprocessing --> OHE/Label Encoder
X = df.drop(labels=['term_deposit_subscribed'],axis=1)
y = df['term_deposit_subscribed']

# Features Scalling
ss = StandardScaler()
X = ss.fit_transform(X)

# Need to save ss model
with open(SS_FILE_NAME,'wb') as file:
  pickle.dump(ss,file)

# OneHotEncoder
ohe = OneHotEncoder(sparse=False)
y = ohe.fit_transform(np.expand_dims(y,axis=1))

# Need to save ohe model
OHE_FILE_NAME = os.path.join(os.getcwd(),'ohe.pkl')

with open(OHE_FILE_NAME,'wb') as file:
  pickle.dump(ohe,file)
              
X_train,X_test,y_train,y_test = train_test_split(X,y,
                                                 test_size=0.3,
                                                 random_state=238)

# Model development
md = ModelCreation()
model = md.model_layer(X_train)

# Wrapping of container
model.compile(optimizer='adam',loss='categorical_crossentropy',
              metrics = ['acc'])

plot_model(model, to_file='model_plot.png', show_shapes=True,
           show_layer_names=True)

# Callbacks
tensorboard_callback = TensorBoard(log_dir=HEART_LOG_FOLDER_PATH)
early_stopping_callback = EarlyStopping(monitor='Loss',patience=3)

# Model training
hist = model.fit(X_train,y_train,validation_data=(X_test,y_test),
                batch_size=128, epochs=100,
                 callbacks=[tensorboard_callback,early_stopping_callback])

with open(MODEL_PATH,'wb') as file:
  pickle.dump(model,file)

#%%
hist.history.keys()

plt.figure()
plt.plot(hist.history['loss'])
plt.plot(hist.history['val_loss'])
plt.legend(['training_loss','validation loss'])
plt.show()

plt.figure()
plt.plot(hist.history['acc'])
plt.plot(hist.history['val_acc'])
plt.legend(['training_accuracy','validation accuracy'])
plt.show()

#%% Model evaluation (after done training)

results = model.evaluate(X_test,y_test)
print(results)
y_true = np.argmax(y_test,axis=1)
y_pred = np.argmax(model.predict(X_test),axis=1)

cr = classification_report(y_true,y_pred)
cm = confusion_matrix(y_true,y_pred)

print(cr)
print(cm)

label = ['0','1']
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=label)
disp.plot(cmap=plt.cm.Blues)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir logs